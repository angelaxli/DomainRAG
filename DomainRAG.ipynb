{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHy6E55asTn0YuFRFX2vAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelaxli/DomainRAG/blob/main/DomainRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project: Domain Name Appraisal using a Retrieval-Augmented Generation Pipeline**\n",
        "\n",
        "Usage:\n",
        "1. Download the historical sales dataset (https://docs.google.com/spreadsheets/d/1SmEplaRY2-a6xt_fAyK1vrtANuZHTqZnXlt5bpmEdHs/edit?gid=0#gid=0).\n",
        "  - The name of your file should be \"Knowledge Base Real (11).csv\".\n",
        "2. Then, you can click Runtime in the menu above and press Run all.\n",
        "3. At the last cell, you should be prompted to input a domain name and the category of the name.\n",
        "  - The category should be one of the listed or simply input Unknown.\n",
        "\n",
        "Note: Still undergoing improvements"
      ],
      "metadata": {
        "id": "W20GRv0yQ0xz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSJmBwH-y1A1",
        "outputId": "e357f3ee-2a74-4cd2-c827-2e909ab603d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting wordsegment\n",
            "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.11/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pytrends) (5.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wordsegment, xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, newsapi-python, multiprocess, pytrends, nvidia-cusolver-cu12, datasets, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 faiss-cpu-1.10.0 multiprocess-0.70.16 newsapi-python-0.2.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytrends-4.9.2 wordsegment-1.3.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas faiss-cpu numpy pytrends transformers datasets torch scikit-learn evaluate wordsegment newsapi-python nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import faiss\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    DPRQuestionEncoder,\n",
        "    DPRContextEncoder,\n",
        "    DPRQuestionEncoderTokenizer,\n",
        "    DPRContextEncoderTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from sklearn.metrics import ndcg_score, average_precision_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import os\n",
        "from scipy.stats import mstats  # Import for winsorizing\n",
        "from transformers import Trainer  # Import the base Trainer class\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import wordsegment\n",
        "from datetime import datetime, timedelta\n",
        "from newsapi.newsapi_exception import NewsAPIException\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "from transformers import AdamW\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.metrics import ndcg_score, average_precision_score\n",
        "import logging\n",
        "import gc #Garbage collection\n",
        "import xgboost as xgb\n",
        "from newsapi import NewsApiClient"
      ],
      "metadata": {
        "id": "upxni2qWzPQI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NLTK Setup (Corrected) ---\n",
        "try:\n",
        "    nltk.data.find('punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')  # Download the complete 'punkt' resource\n",
        "\n",
        "wordsegment.load()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhppu4Bh5CFn",
        "outputId": "d6d9a409-9772-406b-edce-754228885efd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging Setup\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration (Keep your Config class)\n",
        "class Config:\n",
        "    csv_path = \"/content/Knowledge Base Real (11).csv\"  # Your CSV\n",
        "    question_encoder_model = \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    context_encoder_model = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    max_length = 64  # For DPR\n",
        "    output_model_path = \"best_dpr_model.pth\"  # DPR model path (for *loading*)\n",
        "    faiss_index_path = \"dpr_faiss.index\"  # FAISS index path (for *loading*)\n",
        "    news_api_key = \"YOUR_NEWSAPI_ORG_API_KEY\"   # REPLACE WITH YOUR KEY!\n",
        "    flan_t5_model = \"google/flan-t5-base\" # Or \"google/flan-t5-small\", etc.\n",
        "    days_past = 14\n",
        "    xgb_model_path = \"best_xgboost_model.json\" # Path for XGBoost Model\n",
        "    tensorboard_log_dir = \"runs/dpr_training\"\n",
        "\n",
        "    # --- DPR Training Parameters (added) ---\n",
        "    batch_size = 8       # Example batch size - adjust as needed!\n",
        "    epochs = 10          # Example number of epochs\n",
        "    patience = 2        # Early stopping patience\n",
        "    accumulation_steps = 4 # Gradient accumulation steps\n",
        "    learning_rate = 1e-5 # Example learning rate\n",
        "    warmup_steps = 50    # Warmup steps for learning rate scheduler\n",
        "    max_grad_norm = 1.0   # Gradient clipping\n",
        "    log_every = 10     # Log training progress every N steps\n",
        "    num_negatives = 5 # The amount of hard negatives we want.\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# --- Data Preprocessing (Keep your preprocessing) ---\n",
        "try:\n",
        "    df = pd.read_csv(config.csv_path)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Convert 'Date' to datetime objects (CRITICAL for correct trend calculation)\n",
        "    try:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
        "    except ValueError:\n",
        "        try:\n",
        "            df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')  # Handle other formats if needed\n",
        "        except ValueError:\n",
        "            logger.error(\"Could not parse 'Date' column.  Please check the format in your CSV.\")\n",
        "            exit(1)\n",
        "\n",
        "    # Keep original price BEFORE transformations\n",
        "    df['original_price'] = df['Price']\n",
        "\n",
        "    # Winsorize and Log Transform Price (for DPR training data, keep original)\n",
        "    df['Price'] = mstats.winsorize(df['Price'], limits=[0.01, 0.01])\n",
        "    df['Price'] = np.log1p(df['Price']) # Keep for embeddings.\n",
        "\n",
        "    # Normalize price (and other numeric columns if you use them in the context)\n",
        "    price_scaler = MinMaxScaler()\n",
        "    df['Price'] = price_scaler.fit_transform(df[['Price']])\n",
        "\n",
        "     # Normalize 'Monthly Searches' (and other numeric columns if you use them in DPR context)\n",
        "    if 'Monthly Searches' in df.columns:\n",
        "        other_scaler = MinMaxScaler() # Use different scalers\n",
        "        df['Monthly Searches'] = other_scaler.fit_transform(df[['Monthly Searches']])\n",
        "    if 'CPC (Exact)' in df.columns:\n",
        "        other_scaler = MinMaxScaler()  # Reinitialize or use separate scalers for each\n",
        "        df['CPC (Exact)'] = other_scaler.fit_transform(df[['CPC (Exact)']])\n",
        "    if 'CPC (Phrase)' in df.columns:\n",
        "        other_scaler = MinMaxScaler()\n",
        "        df['CPC (Phrase)'] = other_scaler.fit_transform(df[['CPC (Phrase)']])\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"Error: CSV file not found at {config.csv_path}\")\n",
        "    exit(1)"
      ],
      "metadata": {
        "id": "j22a5VL4VQkw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model and Tokenizer Initialization (Keep this) ---\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(config.question_encoder_model)\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(config.context_encoder_model)\n",
        "\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(config.question_encoder_model).to(config.device)\n",
        "context_encoder = DPRContextEncoder.from_pretrained(config.context_encoder_model).to(config.device)\n",
        "\n",
        "# --- Keyword Preprocessing (Keep your function) ---\n",
        "def preprocess_keywords(keywords):\n",
        "    if isinstance(keywords, str):\n",
        "        keywords = keywords.lower()\n",
        "        tokens = context_tokenizer.tokenize(keywords)\n",
        "        tokens = [token for token in tokens if token.isalnum() and token not in context_tokenizer.all_special_tokens]\n",
        "        return context_tokenizer.convert_tokens_to_string(tokens)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "df['Keywords'] = df['Keywords'].apply(preprocess_keywords)\n",
        "\n",
        "# --- Context Creation Function (Keep your function) ---\n",
        "def create_context_string_train(row):\n",
        "    \"\"\"Creates the context string for DPR *TRAINING*, including trend_score.\"\"\"\n",
        "    context_parts = [\n",
        "        f\"Domain: {row.get('Domain Name', 'N/A')}\",\n",
        "        f\"Price: {row.get('Price', 'N/A')}\",  # ORIGINAL price\n",
        "        f\"Keywords: {row.get('Keywords', 'N/A')}\",\n",
        "        f\"Monthly Searches: {row.get('Monthly Searches', 'N/A')}\",\n",
        "        f\"CPC (Exact): {row.get('CPC (Exact)', 'N/A')}\",\n",
        "        f\"CPC (Phrase): {row.get('CPC (Phrase)', 'N/A')}\",\n",
        "        f\"Length: {row.get('Length', 'N/A')}\",\n",
        "        f\"Hyphens: {not row.get('Excludes Hyphens', True)}\",\n",
        "        f\"Category: {row.get('Category', 'N/A')}\",\n",
        "        f\"Trend Score: {row.get('Trend Score', 'N/A')}\"  # Include trend_score\n",
        "    ]\n",
        "    return \", \".join(context_parts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QvUwtm8VfeQ",
        "outputId": "b83663a0-3eae-4d85-c5df-3b8f7a9e7103"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_similarity_based_hard_negatives(df, context_encoder, context_tokenizer, num_negatives=3):\n",
        "    \"\"\"\n",
        "    Generates hard negatives using FAISS.\n",
        "    \"\"\"\n",
        "    context_encoder.eval()  # Ensure eval mode\n",
        "    hard_negatives_by_index = {}\n",
        "    context_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 1. Generate embeddings *individually* for each context\n",
        "        for _, row in df.iterrows():\n",
        "            context = create_context_string_train(row)  # Use the training version\n",
        "            context_input = context_tokenizer(context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length).to(config.device)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
        "                embedding = context_encoder(**context_input).pooler_output.detach().cpu().float().numpy()\n",
        "            if embedding.size > 0:\n",
        "                context_embeddings.append(embedding.flatten())\n",
        "            else:\n",
        "                logger.warning(f\"Empty embedding for row index. Skipping.\")\n",
        "\n",
        "    # 2. Build FAISS index *after* getting all embeddings\n",
        "    context_embeddings = np.array(context_embeddings)\n",
        "\n",
        "    if context_embeddings.size == 0:\n",
        "        logger.warning(\"No valid context embeddings. Returning empty hard_negatives_by_index.\")\n",
        "        return hard_negatives_by_index\n",
        "\n",
        "    index = faiss.IndexFlatL2(context_embeddings.shape[1])\n",
        "    index.add(context_embeddings)\n",
        "\n",
        "    # 3. Find hard negatives\n",
        "    for i, row in df.iterrows():\n",
        "        current_embedding = context_embeddings[i:i + 1]\n",
        "        if current_embedding.size > 0:\n",
        "          D, I = index.search(current_embedding, k=num_negatives + 1)\n",
        "          similar_indices = [idx for idx in I.flatten() if idx != i][:num_negatives]\n",
        "          hard_negatives_by_index[i] = similar_indices\n",
        "\n",
        "    context_encoder.train() # Switch back to training mode\n",
        "    return hard_negatives_by_index\n",
        "\n",
        "\n",
        "def prepare_triplets_with_negatives(df, hard_negatives_by_index=None, epoch=None):\n",
        "    \"\"\"\n",
        "    Prepares triplets with a focus on *much* harder negatives and a warm-up strategy.\n",
        "    Includes extensive diagnostic printing to understand the generated triplets.\n",
        "    \"\"\"\n",
        "    triplets = []\n",
        "    for idx, row in df.iterrows():\n",
        "        question = f\"What is the estimated value of {row['Domain Name']}?\"\n",
        "        positive_context = create_context_string_train(row)  # Use training context\n",
        "\n",
        "        # --- 1. Hard Negatives from FAISS (if available) ---\n",
        "        hard_negatives = []\n",
        "        if hard_negatives_by_index is not None and idx in hard_negatives_by_index:\n",
        "            for hard_negative_idx in hard_negatives_by_index[idx]:\n",
        "                hard_negative = df.iloc[hard_negative_idx]\n",
        "                hard_negatives.append(create_context_string_train(hard_negative))  # Training context\n",
        "\n",
        "\n",
        "        # --- 2. Stricter Fallback (if FAISS doesn't provide enough) ---\n",
        "        # if  epoch is not None and epoch < 2: # Warm up. #Removed warm up\n",
        "        #     random_negative = df.sample(1).iloc[0]\n",
        "        #     random_negative_context = create_context_string_train(random_negative) # Use training context\n",
        "        #     # During warm-up epochs, use *only* a random negative.\n",
        "        #     negatives = [random_negative_context]\n",
        "        #     logger.info(f\"--- Triplet {idx} (WARM-UP EPOCH) ---\")\n",
        "        #     logger.info(f\"  Question: {question}\")\n",
        "        #     logger.info(f\"  Positive Context: {positive_context}\")\n",
        "        #     logger.info(f\"  Random Negative Context: {random_negative_context}\")\n",
        "        #     logger.info(\"-\" * 30)\n",
        "\n",
        "        # else: #After warming up.\n",
        "        if not hard_negatives: # Only if we don't have FAISS negatives\n",
        "            # 1. MUST be the same Category\n",
        "            candidates = df[df['Category'] == row['Category']].copy()  # USE .copy()!\n",
        "\n",
        "            # 2. MUCH Tighter Price Range (on the TRANSFORMED scale)\n",
        "            candidates = candidates[\n",
        "                (candidates['Price'] >= row['Price'] - 0.02) & (candidates['Price'] <= row['Price'] + 0.02)\n",
        "            ]\n",
        "\n",
        "            # 3. Keyword Overlap (Stricter - at least 2 keywords, case-insensitive)\n",
        "            if 'Keywords' in row and isinstance(row['Keywords'], str):\n",
        "                def keyword_overlap_count(x):\n",
        "                    if isinstance(x, str):\n",
        "                        return sum(1 for k in str(x).lower().split() if k in row['Keywords'].lower())\n",
        "                    else:\n",
        "                        return 0\n",
        "                candidates['overlap_count'] = candidates['Keywords'].apply(keyword_overlap_count)\n",
        "                candidates = candidates[candidates['overlap_count'] >= 2] # Require >= 2 keywords\n",
        "                candidates = candidates.drop(columns=['overlap_count'], errors='ignore')\n",
        "\n",
        "\n",
        "            # 4.  Similar Monthly Searches (if available, and after other filters)\n",
        "            if 'Monthly Searches' in df.columns and not candidates.empty:\n",
        "                candidates = candidates[\n",
        "                    (candidates['Monthly Searches'] >= row['Monthly Searches'] - 0.05) &\n",
        "                    (candidates['Monthly Searches'] <= row['Monthly Searches'] + 0.05)\n",
        "                ]\n",
        "            # 5. Similar CPC (if available)\n",
        "            if 'CPC (Exact)' in df.columns and not candidates.empty:\n",
        "              candidates = candidates[\n",
        "                  (candidates['CPC (Exact)'] >= row['CPC (Exact)'] - 0.05) &\n",
        "                  (candidates['CPC (Exact)'] <= row['CPC (Exact)'] + 0.05)\n",
        "              ]\n",
        "            if 'CPC (Phrase)' in df.columns and not candidates.empty:\n",
        "                candidates = candidates[\n",
        "                    (candidates['CPC (Phrase)'] >= row['CPC (Phrase)'] - 0.05) &\n",
        "                    (candidates['CPC (Phrase)'] <= row['CPC (Phrase)'] + 0.05)\n",
        "                ]\n",
        "\n",
        "            # NO LENGTH CHECK - it's a weak signal\n",
        "\n",
        "            if len(candidates) > 1:  # We still have some candidates\n",
        "                hard_negative = candidates.sample(1).iloc[0]\n",
        "                hard_negatives.append(create_context_string_train(hard_negative)) # Use training context\n",
        "\n",
        "            # We are not falling back to a random choice anymore\n",
        "\n",
        "        random_negative = df.sample(1).iloc[0] #Still need random negative\n",
        "        random_negative_context = create_context_string_train(random_negative) # Use training version\n",
        "        negatives = [random_negative_context] + hard_negatives  # Combine for the dataset\n",
        "\n",
        "        triplets.append({\n",
        "            \"question\": question,\n",
        "            \"positive\": positive_context,\n",
        "            \"random_negative\": random_negative_context if 'random_negative_context' in locals() else \"\",\n",
        "            \"hard_negatives\": hard_negatives\n",
        "        })\n",
        "    return triplets"
      ],
      "metadata": {
        "id": "NP9KoAxWV6-b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataset and DataLoader ---\n",
        "class DomainDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        query = question_tokenizer(sample['question'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length)\n",
        "        positive = context_tokenizer(sample['positive'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length)\n",
        "        random_negative = context_tokenizer(sample['random_negative'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length)\n",
        "        hard_negatives = [context_tokenizer(neg, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length) for neg in sample['hard_negatives']]\n",
        "\n",
        "        # Move tensors to device and remove extra dimension\n",
        "        return {\n",
        "            'query': {k: v.squeeze(0).to(config.device) for k, v in query.items()},\n",
        "            'positive': {k: v.squeeze(0).to(config.device) for k, v in positive.items()},\n",
        "            'random_negative': {k: v.squeeze(0).to(config.device) for k, v in random_negative.items()},\n",
        "            'hard_negatives': [{k: v.squeeze(0).to(config.device) for k, v in hard_negative.items()} for hard_negative in hard_negatives]\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    queries = {key: torch.stack([d['query'][key] for d in batch]) for key in batch[0]['query'].keys()}\n",
        "    positives = {key: torch.stack([d['positive'][key] for d in batch]) for key in batch[0]['positive'].keys()}\n",
        "    random_negatives = {key: torch.stack([d['random_negative'][key] for d in batch]) for key in batch[0]['random_negative'].keys()}\n",
        "    hard_negatives_list = []\n",
        "    num_hard_negatives = min(len(d['hard_negatives']) for d in batch)\n",
        "    for i in range(num_hard_negatives):\n",
        "        hard_negatives_list.append({key: torch.stack([d['hard_negatives'][i][key] for d in batch]) for key in batch[0]['hard_negatives'][i].keys()})\n",
        "    return queries, positives, random_negatives, hard_negatives_list\n",
        "\n",
        "def train_step(query, positive, random_negative, hard_negatives, optimizer, criterion, scaler):\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "    # Force float32 for both forward and backward passes\n",
        "    with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
        "        query_embedding = question_encoder(**query).pooler_output\n",
        "        positive_embedding = context_encoder(**positive).pooler_output\n",
        "        random_negative_embedding = context_encoder(**random_negative).pooler_output\n",
        "\n",
        "        scores_list = [torch.matmul(query_embedding, positive_embedding.T).diag(),\n",
        "                       torch.matmul(query_embedding, random_negative_embedding.T).diag()]\n",
        "\n",
        "        for hard_negative in hard_negatives:\n",
        "            hard_negative_embedding = context_encoder(**hard_negative).pooler_output\n",
        "            scores_list.append(torch.matmul(query_embedding, hard_negative_embedding.T).diag())\n",
        "\n",
        "        scores = torch.stack(scores_list, dim=1)\n",
        "\n",
        "        batch_size = query_embedding.size(0)\n",
        "        targets = torch.zeros(batch_size, dtype=torch.long, device=config.device)\n",
        "\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "    # Continue with gradient scaling and optimization\n",
        "    scaler.scale(loss).backward()  # Scale the loss\n",
        "    scaler.unscale_(optimizer) # Unscale before clipping\n",
        "    torch.nn.utils.clip_grad_norm_(question_encoder.parameters(), max_norm=config.max_grad_norm)\n",
        "    torch.nn.utils.clip_grad_norm_(context_encoder.parameters(), max_norm=config.max_grad_norm)\n",
        "    scaler.step(optimizer)  # Step with scaler\n",
        "    scaler.update()  # Update scaler\n",
        "\n",
        "    with torch.no_grad(): # Calculate MRR (no gradient needed)\n",
        "        ranks = torch.argsort(scores, dim=1, descending=True)\n",
        "        positive_indices = (ranks == 0).nonzero(as_tuple=True)\n",
        "        if len(positive_indices[0]) > 0:\n",
        "            positive_ranks = positive_indices[1] + 1\n",
        "            mrr = torch.mean(1.0 / positive_ranks.float())\n",
        "        else:\n",
        "            mrr = torch.tensor(0.0, device=config.device)\n",
        "\n",
        "    return loss.detach(), mrr.detach() # Return detached losses\n"
      ],
      "metadata": {
        "id": "O2N8428UV8QE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(dataloader, question_encoder, context_encoder):\n",
        "    question_encoder.eval()\n",
        "    context_encoder.eval()\n",
        "\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            query, positive, random_negative, hard_negatives = batch\n",
        "\n",
        "            query_embedding = question_encoder(**query).pooler_output\n",
        "            positive_embedding = context_encoder(**positive).pooler_output\n",
        "\n",
        "            scores_list = [torch.matmul(query_embedding, positive_embedding.T).diag()]\n",
        "            for negative in [random_negative] + hard_negatives:\n",
        "                negative_embedding = context_encoder(**negative).pooler_output\n",
        "                scores_list.append(torch.matmul(query_embedding, negative_embedding.T).diag())\n",
        "\n",
        "            scores = torch.stack(scores_list, dim=1)\n",
        "\n",
        "            all_scores.extend(scores.cpu().float().numpy())  # Convert to float32\n",
        "            all_labels.extend(torch.zeros(scores.shape[0], dtype=torch.int64).cpu().numpy())\n",
        "\n",
        "    all_scores = np.array(all_scores)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate MRR\n",
        "    ranks = np.argsort(all_scores, axis=1)[:, ::-1]\n",
        "    positive_ranks = (ranks == 0).nonzero()[1] + 1 if np.any(ranks == 0) else np.array([])\n",
        "    mrr = np.mean(1.0 / positive_ranks) if len(positive_ranks) > 0 else 0.0\n",
        "\n",
        "    question_encoder.train()\n",
        "    context_encoder.train()\n",
        "    return mrr"
      ],
      "metadata": {
        "id": "U5CDohqKWD_L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Loop ---\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "num_training_steps = (len(train_df) // config.batch_size // config.accumulation_steps) * config.epochs\n",
        "optimizer = AdamW(list(question_encoder.parameters()) + list(context_encoder.parameters()), lr=config.learning_rate)\n",
        "# Re-enable warmup steps:\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=num_training_steps)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(config.device.type == 'cuda'))\n",
        "\n",
        "writer = SummaryWriter(log_dir=config.tensorboard_log_dir)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_model_state_dict = None\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "    # Generate hard negatives every epoch\n",
        "    hard_negatives_by_index = generate_similarity_based_hard_negatives(train_df, context_encoder, context_tokenizer, num_negatives=config.num_negatives)\n",
        "\n",
        "\n",
        "    train_triplets = prepare_triplets_with_negatives(train_df, hard_negatives_by_index, epoch)  # Pass hard_negatives_by_index\n",
        "    train_dataset = DomainDataset(train_triplets)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    #Hard negatives are optional for val.\n",
        "    val_triplets = prepare_triplets_with_negatives(val_df)\n",
        "    val_dataset = DomainDataset(val_triplets)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        loss, mrr = train_step(batch[0], batch[1], batch[2], batch[3], optimizer, criterion, scaler)\n",
        "\n",
        "        if (step + 1) % config.accumulation_steps == 0:\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad() #Only zero when accumulated.\n",
        "\n",
        "        if step % config.log_every == 0:\n",
        "            logger.info(f\"Epoch {epoch}, Step {step}, Loss: {loss.item()}, MRR: {mrr.item()}\")\n",
        "            writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_dataloader) + step)\n",
        "            writer.add_scalar(\"MRR/train\", mrr.item(), epoch * len(train_dataloader) + step)\n",
        "\n",
        "    val_mrr = evaluate_model(val_dataloader, question_encoder, context_encoder)\n",
        "    val_loss = 1.0 - val_mrr # Lower is better\n",
        "\n",
        "\n",
        "    logger.info(f\"Epoch {epoch+1}, Validation MRR: {val_mrr:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "    writer.add_scalar(\"MRR/val\", val_mrr, epoch)\n",
        "    writer.add_scalar(\"Loss/val\", val_loss, epoch) # Log val_loss\n",
        "\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state_dict = {\n",
        "            'question_encoder': question_encoder.state_dict(),\n",
        "            'context_encoder': context_encoder.state_dict()\n",
        "        }\n",
        "        torch.save(best_model_state_dict, config.output_model_path)\n",
        "        logger.info(f\"Best model saved at epoch {epoch+1}\")\n",
        "\n",
        "    elif epoch >= config.patience and (val_loss >= best_val_loss):\n",
        "        logger.info(f\"Early stopping triggered. No improvement for {config.patience} epochs.\")\n",
        "        break\n",
        "\n",
        "# Load the best model after training\n",
        "if best_model_state_dict is not None:\n",
        "    best_model_state_dict = torch.load(config.output_model_path, map_location=config.device)\n",
        "    question_encoder.load_state_dict(best_model_state_dict['question_encoder'])\n",
        "    context_encoder.load_state_dict(best_model_state_dict['context_encoder'])\n",
        "    logger.info(\"Best model loaded.\")\n",
        "else:\n",
        "    logger.warning(\"Warning: No best model found. This can happen if training was interrupted.\")\n",
        "\n",
        "# --- FAISS Index Building (After Training) ---\n",
        "\n",
        "context_encoder.eval()\n",
        "index = faiss.IndexFlatL2(768)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, row in df.iterrows():\n",
        "        context = create_context_string_train(row)\n",
        "        context_input = context_tokenizer(context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length).to(config.device)\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
        "          embedding = context_encoder(**context_input).pooler_output.detach().cpu().float().numpy()\n",
        "\n",
        "        if embedding.size > 0:\n",
        "              index.add(embedding.reshape(1, -1)) #FAISS needs 2D array.\n",
        "        else:\n",
        "            logger.warning(f\"Empty embedding for row index {idx}. Skipping.\")\n",
        "\n",
        "faiss.write_index(index, config.faiss_index_path)\n",
        "logger.info(f\"FAISS index built and saved to {config.faiss_index_path}\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "writer.close()\n",
        "\n",
        "print(\"Training and Index Building complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "dk0loUtYWvd4",
        "outputId": "3842ae74-b777-4d53-cbbe-550d036bd2a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-9-bba88fdddec4>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(config.device.type == 'cuda'))\n",
            "<ipython-input-6-6f8d49d96ba0>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-7-eaf5fb33a2d3>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-6-6f8d49d96ba0>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-7-eaf5fb33a2d3>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-6-6f8d49d96ba0>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-7-eaf5fb33a2d3>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-6-6f8d49d96ba0>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-7-eaf5fb33a2d3>:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n",
            "<ipython-input-9-bba88fdddec4>:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model_state_dict = torch.load(config.output_model_path, map_location=config.device)\n",
            "<ipython-input-9-bba88fdddec4>:83: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float32, enabled=(config.device.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and Index Building complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "index = faiss.read_index(\"dpr_faiss.index\")"
      ],
      "metadata": {
        "id": "0yslZ2pCBx_U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Index is trained: {index.is_trained}\")  # Should be True\n",
        "print(f\"Number of vectors in index: {index.ntotal}\")  # Should match # of rows in CSV\n",
        "print(f\"Dimension of vectors: {index.d}\")      # Should be 768 (for DPR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmp26JRrCGHH",
        "outputId": "6d6cfcd9-f683-438b-8c6e-92468ab4fa64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index is trained: True\n",
            "Number of vectors in index: 149\n",
            "Dimension of vectors: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "class Config:\n",
        "    csv_path = \"/content/Knowledge Base Real (11).csv\"  # Your CSV\n",
        "    question_encoder_model = \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "    context_encoder_model = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    max_length = 64  # For DPR\n",
        "    output_model_path = \"best_dpr_model.pth\"  # DPR model path (for *loading*)\n",
        "    faiss_index_path = \"dpr_faiss.index\"  # FAISS index path (for *loading*)\n",
        "    news_api_key = \"e8be2b7d92b147a686526909dcdfc648\"   # REPLACE WITH YOUR KEY!\n",
        "    flan_t5_model = \"google/flan-t5-base\" # Or \"google/flan-t5-small\", etc.\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "gQpznUBIGcmF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Keyword Extraction (Final Version) ---\n",
        "def extract_keywords(domain_name):\n",
        "    \"\"\"Extracts keywords from a domain name.\"\"\"\n",
        "    domain_without_tld = domain_name.split('.')[0]\n",
        "    parts = re.split(r'[-_]+', domain_without_tld)\n",
        "    keywords = []\n",
        "\n",
        "    for part in parts:\n",
        "        if not part.isdigit():\n",
        "            try:\n",
        "                segmented_words = wordsegment.segment(part)\n",
        "                keywords.extend([word.lower() for word in segmented_words if len(word) > 1])\n",
        "            except Exception as e:\n",
        "                print(f\"Word segmentation failed for '{part}': {e}.  Falling back.\")\n",
        "                sub_parts = re.findall(r'[a-zA-Z]+|[0-9]+', part)\n",
        "                for sub_part in sub_parts:\n",
        "                    if not sub_part.isdigit():\n",
        "                        try:\n",
        "                            segmented_sub_words = wordsegment.segment(sub_part)\n",
        "                            keywords.extend([w.lower() for w in segmented_sub_words if len(w) > 1])\n",
        "                        except Exception as e2:\n",
        "                            print(f\"  Sub-segmentation failed for '{sub_part}': {e2}. Keeping as is (if > 1).\")\n",
        "                            if len(sub_part) > 1:\n",
        "                                keywords.append(sub_part.lower())\n",
        "                    else:\n",
        "                        keywords.append(sub_part)\n",
        "        else:\n",
        "            keywords.append(part)\n",
        "\n",
        "    cleaned_domain_name = domain_without_tld.replace(\"-\", \" \").replace(\"_\", \" \").strip()\n",
        "\n",
        "    if cleaned_domain_name.lower() != \" \".join(keywords).lower():\n",
        "        keywords.append(cleaned_domain_name)\n",
        "\n",
        "    unique_keywords = []\n",
        "    for keyword in keywords:\n",
        "        if keyword not in unique_keywords:\n",
        "            unique_keywords.append(keyword)\n",
        "\n",
        "    return list(unique_keywords)  # Return as a *list*\n",
        "\n",
        "# --- NewsAPI Integration ---\n",
        "def get_news_article_count(api_key, keywords, days_past=7, cache_dir=\"news_counts_cache\"):\n",
        "    \"\"\"Fetches news article count (past 14 days).\"\"\"\n",
        "    newsapi = NewsApiClient(api_key=api_key)\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    if not keywords:\n",
        "        print(\"No keywords extracted. Returning article count of 0.\")\n",
        "        return 0\n",
        "\n",
        "    query_string = \" OR \".join(keywords)\n",
        "    cache_key = query_string.replace(\" \", \"_\") + f\"_{days_past}\"\n",
        "    cache_file = os.path.join(cache_dir, f\"{cache_key}.json\")\n",
        "\n",
        "    if os.path.exists(cache_file):\n",
        "        try:\n",
        "            with open(cache_file, 'r') as f:\n",
        "                cached_data = json.load(f)\n",
        "            if datetime.fromisoformat(cached_data['timestamp']).date() >= (datetime.now() - timedelta(days=1)).date():\n",
        "                print(f\"Using cached news count for query: {query_string}\")\n",
        "                return cached_data['count']\n",
        "        except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:\n",
        "            print(f\"Error reading cache for '{query_string}': {e}.  Ignoring cache.\")\n",
        "\n",
        "    try:\n",
        "        from_date = (datetime.now() - timedelta(days=days_past)).strftime('%Y-%m-%d')\n",
        "        retries = 3\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = newsapi.get_everything(q=query_string,\n",
        "                                                  from_param=from_date,\n",
        "                                                  language='en',\n",
        "                                                  sort_by='relevancy',\n",
        "                                                  page_size=1,\n",
        "                                                  page=1)\n",
        "\n",
        "                if response['status'] == 'ok':\n",
        "                    article_count = response['totalResults']\n",
        "                    article_count = min(article_count, 100)\n",
        "\n",
        "                    with open(cache_file, 'w') as f:\n",
        "                        json.dump({'timestamp': datetime.now().isoformat(), 'count': article_count}, f)\n",
        "\n",
        "                    return article_count\n",
        "\n",
        "                elif response['code'] == 'rateLimited':\n",
        "                    if attempt == retries - 1:\n",
        "                        print(f\"Rate limited after {retries} attempts for query: {query_string}.\")\n",
        "                        return -1\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"Rate limited.  Waiting {wait_time} seconds before retrying...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"NewsAPI error for '{query_string}': {response['code']} - {response['message']}\")\n",
        "                    return -1\n",
        "\n",
        "            except NewsAPIException as e:\n",
        "                print(f\"NewsAPI Exception for '{query_string}': {e}\")\n",
        "                return -1\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error for '{query_string}': {e}\")\n",
        "                return -1\n",
        "    except Exception as e:\n",
        "        print(f\"Date Calculation Error: {e}\")\n",
        "        return -1\n",
        "\n",
        "    return -1\n",
        "\n",
        "def calculate_trend_score(domain_name, article_count):\n",
        "    \"\"\"Calculates a trend score.\"\"\"\n",
        "    if article_count == -1:\n",
        "        return 0.0\n",
        "\n",
        "    score = 0\n",
        "    tld = domain_name.split('.')[-1].lower()\n",
        "    if tld == 'com':\n",
        "        score += 3\n",
        "    elif tld == '.ai':\n",
        "        score += 2\n",
        "    elif tld in ('net', 'org'):\n",
        "        score += 1\n",
        "\n",
        "    if '-' in domain_name:\n",
        "        score -= 1\n",
        "    if '_' in domain_name:\n",
        "        score -= 1\n",
        "    if any(char.isdigit() for char in domain_name):\n",
        "        score -= 1\n",
        "\n",
        "    normalized_article_count = min(article_count, 100) / 100.0\n",
        "    score += normalized_article_count * 5\n",
        "    return score"
      ],
      "metadata": {
        "id": "kS-MW2XxHFIE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DPR Retrieval ---\n",
        "def load_dpr_components(config):\n",
        "    \"\"\"Loads trained DPR model and FAISS index.\"\"\"\n",
        "    question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(config.question_encoder_model)\n",
        "    context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(config.context_encoder_model)\n",
        "    question_encoder = DPRQuestionEncoder.from_pretrained(config.question_encoder_model).to(config.device)\n",
        "    context_encoder = DPRContextEncoder.from_pretrained(config.context_encoder_model).to(config.device)\n",
        "\n",
        "    checkpoint = torch.load(config.output_model_path, map_location=config.device)\n",
        "    question_encoder.load_state_dict(checkpoint['question_encoder'])\n",
        "    context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
        "    question_encoder.eval()\n",
        "    context_encoder.eval()\n",
        "\n",
        "    index = faiss.read_index(config.faiss_index_path)\n",
        "    return question_encoder, context_encoder, index, question_tokenizer, context_tokenizer\n",
        "\n",
        "def retrieve_comparable_sales(domain_name, keywords, question_encoder, context_encoder, index, context_tokenizer, user_category):\n",
        "    \"\"\"Retrieves comparable sales using DPR, filtering by category.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        dpr_input = domain_name + \" \" + \" \".join(keywords)\n",
        "        query_input = question_tokenizer(dpr_input, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=config.max_length).to(config.device)\n",
        "        query_embedding = question_encoder(**query_input).pooler_output.cpu().numpy()\n",
        "\n",
        "    k = 20  # Retrieve more initially, then filter\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    comparable_sales = []\n",
        "    for i in indices[0]:\n",
        "        domain, price, category = get_domain_data_from_index(i)  # Get category\n",
        "        if category == user_category:  # Filter by category!\n",
        "            comparable_sales.append((domain, price))\n",
        "            if len(comparable_sales) >= 5:  # Limit to 5 *after* filtering\n",
        "                break\n",
        "\n",
        "    return comparable_sales\n",
        "\n",
        "# --- MODIFIED: Load DataFrame ONCE ---\n",
        "_cached_df = None  # Global variable to store the DataFrame\n",
        "\n",
        "def get_domain_data_from_index(index_value):\n",
        "    \"\"\"Retrieves original domain data from the CSV, given a FAISS index.\"\"\"\n",
        "    global _cached_df  # Use the global variable\n",
        "    if _cached_df is None:\n",
        "        # Load the DataFrame only once\n",
        "        _cached_df = pd.read_csv(config.csv_path)\n",
        "        print(\"DataFrame loaded into memory.\")  # Indicate loading\n",
        "    row = _cached_df.iloc[index_value]\n",
        "    return row['Domain Name'], row['Price'], row['Category']  # ORIGINAL Price!\n",
        "\n",
        "\n",
        "# --- Feature Engineering ---\n",
        "def engineer_features(domain_name, trend_score, comparable_sales, user_category, df_row=None):\n",
        "    features = {\n",
        "        'domain_length': len(domain_name),\n",
        "        'tld': domain_name.split('.')[-1].lower(),\n",
        "        'trend_score': trend_score,  # Use the passed-in trend_score\n",
        "        'contains_numbers': any(char.isdigit() for char in domain_name),\n",
        "        'contains_hyphens': '-' in domain_name,\n",
        "        'avg_comparable_price': 0.0,\n",
        "        'max_comparable_price': 0.0,\n",
        "        'min_comparable_price': 0.0,\n",
        "        'monthly_searches': 0.0,  # Default values\n",
        "        'cpc_exact': 0.0,\n",
        "        'cpc_phrase': 0.0,\n",
        "        'user_category': user_category,\n",
        "    }\n",
        "\n",
        "    if comparable_sales:\n",
        "        prices = [price for _, price in comparable_sales]\n",
        "        features['avg_comparable_price'] = sum(prices) / len(prices)\n",
        "        features['max_comparable_price'] = max(prices)\n",
        "        features['min_comparable_price'] = min(prices)\n",
        "\n",
        "    # Add original features from the dataset (if available - during training)\n",
        "    if df_row is not None:\n",
        "        # features['monthly_searches'] = df_row.get('Monthly Searches', 0.0) # REMOVED\n",
        "        pass # Removed\n",
        "\n",
        "    return features\n",
        "\n",
        "    # --- FLAN-T5 Explanation Generation ---\n",
        "def generate_explanation_flan_t5(domain_name, predicted_price, features, comparable_sales):\n",
        "    \"\"\"Generates an explanation using FLAN-T5 (Few-Shot Prompt).\"\"\"\n",
        "\n",
        "    # --- Few-Shot Examples (CRUCIAL for good explanations) ---\n",
        "    examples = \"\"\"\n",
        "Example 1:\n",
        "Domain: bestshoes.com\n",
        "Predicted Price: $1850.00\n",
        "Domain Features:\n",
        "- Length: 12\n",
        "- TLD: com\n",
        "- Trend Score: 4.20\n",
        "- Contains Numbers: False\n",
        "- Contains Hyphens: False\n",
        "- Category: Fashion\n",
        "Comparable Sales:\n",
        "- shoesite.com: $1700.00\n",
        "- footwearplace.com: $2000.00\n",
        "Explanation: bestshoes.com is a short and memorable .com domain, which makes it valuable.  The trend score of 4.20 indicates good interest in related topics.  The price is also consistent with comparable sales of similar domains.\n",
        "\n",
        "Example 2:\n",
        "Domain: long-domain-name-123.net\n",
        "Predicted Price: $120.00\n",
        "Domain Features:\n",
        "- Length: 23\n",
        "- TLD: net\n",
        "- Trend Score: 0.80\n",
        "- Contains Numbers: True\n",
        "- Contains Hyphens: True\n",
        "- Category: Technology\n",
        "Comparable Sales:\n",
        "- No comparable sales found.\n",
        "Explanation:  The predicted price of $120 for long-domain-name-123.net is low due to several factors. The domain is quite long, contains hyphens and numbers (which are generally undesirable), and has a .net TLD, which is less valuable than .com. The low trend score suggests limited current interest, and no close comparable sales were found.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{examples}\n",
        "Now, explain the predicted price of the domain name '{domain_name}'.\n",
        "\n",
        "Predicted Price: ${predicted_price:.2f}\n",
        "\n",
        "Domain Features:\n",
        "- Length: {features['domain_length']}\n",
        "- TLD: {features['tld']}\n",
        "- Trend Score: {features['trend_score']:.2f}\n",
        "- Contains Numbers: {features['contains_numbers']}\n",
        "- Contains Hyphens: {features['contains_hyphens']}\n",
        "- Category: {features['user_category']}\n",
        "\n",
        "Comparable Sales:\n",
        "\"\"\"\n",
        "    if comparable_sales:\n",
        "        for comp_domain, comp_price in comparable_sales:\n",
        "            prompt += f\"- {comp_domain}: ${comp_price:.2f}\\n\"\n",
        "    else:\n",
        "        prompt += \"- No comparable sales found.\\n\"\n",
        "\n",
        "    prompt += \"\\nExplanation:\"\n",
        "\n",
        "    # --- Debugging: Print the prompt ---\n",
        "    print(\"----- FLAN-T5 PROMPT -----\")\n",
        "    print(prompt)\n",
        "    print(\"----- END PROMPT -----\")\n",
        "\n",
        "    # --- Tokenize and Generate ---\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        # Use beam search and adjust parameters\n",
        "        outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95) # Parameters\n",
        "        # print(f\"Output IDs: {outputs}\")  # Debug Print\n",
        "    explanation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return explanation"
      ],
      "metadata": {
        "id": "YauZw44kHOtJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prediction Function (XGBoost + FLAN-T5) ---\n",
        "def predict_price(domain_name, user_category, xgb_model, train_columns, question_encoder, context_encoder, index, context_tokenizer):\n",
        "    \"\"\"Predicts the price of a domain name.\"\"\"\n",
        "    keywords = extract_keywords(domain_name)\n",
        "    article_count = get_news_article_count(config.news_api_key, keywords, config.days_past)\n",
        "    trend_score = calculate_trend_score(domain_name, article_count)\n",
        "    comparable_sales = retrieve_comparable_sales(domain_name, keywords, question_encoder, context_encoder, index, context_tokenizer, user_category)\n",
        "\n",
        "    # --- Feature Engineering for Prediction (Corrected) ---\n",
        "    features = engineer_features(domain_name, trend_score, comparable_sales, user_category)\n",
        "\n",
        "    # Create a DataFrame for the *single* input domain\n",
        "    input_df = pd.DataFrame([features])\n",
        "\n",
        "    # --- One-Hot Encode (Consistent with Training) ---\n",
        "    input_df = pd.get_dummies(input_df, columns=['tld', 'user_category'], prefix=['tld', 'cat'], dummy_na=False)\n",
        "\n",
        "    # --- Align Columns (CRITICAL) ---\n",
        "    # Add missing columns (if any) and set them to 0\n",
        "    missing_cols = set(train_columns) - set(input_df.columns)\n",
        "    for c in missing_cols:\n",
        "        input_df[c] = 0\n",
        "    # Ensure the order of columns is the same as in training\n",
        "    input_df = input_df[train_columns]\n",
        "\n",
        "    # Predict using XGBoost\n",
        "    predicted_price = xgb_model.predict(input_df)[0]  # Get the single prediction\n",
        "\n",
        "\n",
        "    # Generate explanation (FLAN-T5) - you'll need to implement this part\n",
        "    explanation = generate_explanation_flan_t5(domain_name, predicted_price, features, comparable_sales)\n",
        "\n",
        "    return predicted_price, explanation\n",
        "\n",
        "\n",
        "# --- Data Loading and Preprocessing for XGBoost Training ---\n",
        "def load_and_preprocess_data(csv_path):\n",
        "    \"\"\"Loads and preprocesses data for XGBoost, including original features.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df = df.dropna()  # Handle missing values\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # --- Feature Engineering (Include original features) ---\n",
        "        df['domain_length'] = df['Domain Name'].apply(len)\n",
        "        df['contains_numbers'] = df['Domain Name'].apply(lambda x: any(char.isdigit() for char in x))\n",
        "        df['contains_hyphens'] = df['Domain Name'].apply(lambda x: '-' in x)\n",
        "        df['tld'] = df['Domain Name'].apply(lambda x: x.split('.')[-1].lower())\n",
        "        # Select features and target (include original features, exclude 'Domain Name')\n",
        "        features = ['domain_length', 'contains_numbers', 'contains_hyphens',\n",
        "                    'Monthly Searches', 'CPC (Exact)', 'CPC (Phrase)','tld', 'Category', 'Trend Score', 'Price']  # Include original features\n",
        "\n",
        "\n",
        "        # Ensure all required columns exist\n",
        "        for col in features:\n",
        "            if col not in df.columns:\n",
        "                print(f\"Error: Required column '{col}' not found in CSV.\")\n",
        "                exit(1)  # Or handle more gracefully\n",
        "\n",
        "        # --- One-Hot Encode 'tld' and 'Category' *BEFORE* separating features and target ---\n",
        "        df = pd.get_dummies(df, columns=['tld', 'Category'], prefix=['tld', 'cat'], dummy_na=False)\n",
        "\n",
        "        # Drop domain name\n",
        "        df = df.drop(columns=['Domain Name'])\n",
        "\n",
        "        #Separate features and target\n",
        "        y = df['Price']\n",
        "        X = df.drop(columns=['Price'])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at {csv_path}\")\n",
        "        return None, None  # Return None, None on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading/preprocessing data: {e}\")\n",
        "        return None, None  # Return None, None on error\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at {csv_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading/preprocessing data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at {csv_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading/preprocessing data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- Model Training (XGBoost) ---\n",
        "def train_xgboost_model(X, y):\n",
        "    \"\"\"Trains an XGBoost model.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Mean Absolute Error: {mae}\")\n",
        "    print(f\"R-squared: {r2}\")\n",
        "\n",
        "    return model, X_train.columns"
      ],
      "metadata": {
        "id": "QWkBZkD_HbBM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    api_key = config.news_api_key\n",
        "    if not api_key:\n",
        "        print(\"Please set your NewsAPI key in config.news_api_key\")\n",
        "        exit(1)\n",
        "\n",
        "    # --- 1. Load Data and Train XGBoost Model ---\n",
        "    X, y = load_and_preprocess_data(config.csv_path)  # Load and preprocess\n",
        "    if X is None or y is None:\n",
        "        print(\"Error: Could not load or preprocess data. Exiting.\")\n",
        "        exit(1)\n",
        "\n",
        "    xgb_model, train_columns = train_xgboost_model(X, y)  # Train XGBoost\n",
        "\n",
        "    # --- 2. Load FLAN-T5 Model and Tokenizer ---\n",
        "    tokenizer = T5Tokenizer.from_pretrained(config.flan_t5_model)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(config.flan_t5_model).to(config.device)\n",
        "    model.eval()\n",
        "    print(f\"FLAN-T5 model loaded on device: {model.device}\")  # CHECK DEVICE\n",
        "    print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")  # CHECK TOKENIZER\n",
        "\n",
        "    # --- 3. Load DPR Model and FAISS Index ---\n",
        "    question_encoder, context_encoder, dpr_index, question_tokenizer, context_tokenizer = load_dpr_components(config)\n",
        "\n",
        "    # --- 4. Get User Input and Make Prediction ---\n",
        "    domain_name = input(\"Enter the domain name to appraise: \")\n",
        "    user_category = input(\"Enter the category of the domain: \") #Added input\n",
        "\n",
        "\n",
        "    predicted_price, explanation = predict_price(\n",
        "        domain_name, xgb_model, question_encoder, context_encoder, dpr_index, context_tokenizer,\n",
        "        config.news_api_key, tokenizer, model, user_category  # Pass user_category\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Results ---\")\n",
        "    print(explanation)\n",
        "    print(f\"Predicted price: {predicted_price}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "c3tY58qyFjul",
        "outputId": "b710e76c-b862-4c29-b7f2-6b9df5b1d6f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Date: object, Keywords: object, TLD: object, Excludes Hyphens: object, Excludes Numbers: object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-dfd37aac3666>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_xgboost_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train XGBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# --- 2. Load FLAN-T5 Model and Tokenizer ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-90409bd155f0>\u001b[0m in \u001b[0;36mtrain_xgboost_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reg:squarederror'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalsLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[1;32m    602\u001b[0m     way.\"\"\"\n\u001b[0;32m--> 603\u001b[0;31m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"gblinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                 return QuantileDMatrix(\n\u001b[0m\u001b[1;32m   1066\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1571\u001b[0m                 )\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         self._init(\n\u001b[0m\u001b[1;32m   1574\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         )\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m  \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# Defer the exception in order to return 0 and stop the iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         \u001b[0minput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minput_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temporary_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 new, cat_codes, feature_names, feature_types = _proxy_transform(\n\u001b[0m\u001b[1;32m    618\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_proxy_transform\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1445\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arrow_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m         df, feature_names, feature_types = _transform_pandas_df(\n\u001b[0m\u001b[1;32m   1448\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0mmeta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m ) -> Tuple[PandasTransformed, Optional[FeatureNames], Optional[FeatureTypes]]:\n\u001b[0;32m--> 603\u001b[0;31m     \u001b[0mpandas_check_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_matrix_meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"DataFrame for {meta} cannot have multiple columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36mpandas_check_dtypes\u001b[0;34m(data, enable_categorical)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mis_pa_ext_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         ):\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0m_invalid_dataframe_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_pd_sparse_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_invalid_dataframe_dtype\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DataFrame.dtypes for data must be int, float, bool or category.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\"\"{type_err} {_ENABLE_CAT_ERR} {err}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Date: object, Keywords: object, TLD: object, Excludes Hyphens: object, Excludes Numbers: object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "importances = xgb_model.feature_importances_\n",
        "feature_names = train_columns  # Your training data's column names\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "qf-3d5Fqs045"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}